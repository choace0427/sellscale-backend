{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai_api_key = 'sk-'\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preview': \"I've really been impressed after reading up on your work at Self Financial. It seems like you're leading the growth marketing efforts which is amazing. I'm curious to know more about how you're leading the charge given your impressive background with other awesome brands.\", 'status': 200}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "example_completion=\"Hey Chris! I read your profile and I wanted to send a quick note to say hello. I also wanted to let you know that I've heard of Levels.fyi - I think your company is really interesting and I have to say I'm a bit jealous that you're in charge of comp & benefits at Reddit!  \"\n",
    "example_fix=\"The completion needs to ask Chris if they have heard about Levels.fyi, not share that 'I've heard of Levels.fyi.'\"\n",
    "example_completion2=\"I've really been impressed after reading up on your work at Self Financial. It seems like you're leading the growth marketing efforts which is amazing given the context. I'm curious to know more about how you're leading the charge given your impressive background with other awesome brands.  \"\n",
    "example_fix2=\"Remove references to hardship, specifically 'given the context.'\"\n",
    "\n",
    "\n",
    "def wrapped_chat_gpt_completion(\n",
    "    messages: list,\n",
    "    history: Optional[list] = [],\n",
    "    max_tokens: Optional[int] = 100,\n",
    "    temperature: Optional[float] = 0.75,\n",
    "    top_p: Optional[float] = 1,\n",
    "    n: Optional[int] = 1,\n",
    "    frequency_penalty: Optional[float] = 0,\n",
    "):\n",
    "    if history:\n",
    "        messages = history + messages\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model='gpt-3.5-turbo',\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        n=n,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "    if response is None or response[\"choices\"] is None or len(response[\"choices\"]) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    choices = response[\"choices\"]\n",
    "    top_choice = choices[0]\n",
    "    preview = top_choice[\"message\"][\"content\"].strip()\n",
    "\n",
    "    messages = messages + [{\"role\": \"assistant\", \"content\": preview}]\n",
    "    return messages, preview\n",
    "\n",
    "\n",
    "def preview_fix(completion: str, fix: str):\n",
    "    \"\"\" Previews the fix for a given completion.\n",
    "\n",
    "    Args:\n",
    "        completion (str): Completion to preview the fix for.\n",
    "        fix (str): Fix to preview.\n",
    "\n",
    "    Returns:\n",
    "        str: Preview of the fix.\n",
    "    \"\"\"\n",
    "    # Define the maximum number of generated tokens to be equal to 1.25 the original message.\n",
    "    max_tokens_length = int(len(completion) * 1.25)\n",
    "    completion = completion.strip()\n",
    "    fix = fix.strip()\n",
    "    \n",
    "    messages, preview = wrapped_chat_gpt_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"completion: {}\\nfix: {}\\ncompletion:\".format(completion, fix)},\n",
    "        ],\n",
    "        max_tokens=max_tokens_length,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return preview, 200\n",
    "\n",
    "returned = preview_fix(example_completion2, example_fix2)\n",
    "print(returned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60a39ed7d9ed506b3549da2e10402e1e4204e8b41b0e183e3e35940f3cb41bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
