{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import sys\n",
    "import openai\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from src/ml/openai_wrappers.py\n",
    "from ctypes import Union\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "CURRENT_OPENAI_DAVINCI_MODEL = \"text-davinci-003\"\n",
    "CURRENT_OPENAI_CHAT_GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "CURRENT_OPENAI_LATEST_GPT_MODEL = \"gpt-4\"\n",
    "DEFAULT_SUFFIX = None\n",
    "DEFAULT_MAX_TOKENS = 16\n",
    "DEFAULT_TEMPERATURE = 1\n",
    "DEFAULT_TOP_P = 1\n",
    "DEFAULT_N = 1\n",
    "DEFAULT_FREQUENCY_PENALTY = 0\n",
    "DEFAULT_STOP = None\n",
    "\n",
    "def wrapped_chat_gpt_completion(\n",
    "    messages: list,\n",
    "    history: Optional[list] = [],\n",
    "    max_tokens: Optional[int] = DEFAULT_MAX_TOKENS,\n",
    "    temperature: Optional[float] = DEFAULT_TEMPERATURE,\n",
    "    top_p: Optional[float] = DEFAULT_TOP_P,\n",
    "    n: Optional[int] = DEFAULT_N,\n",
    "    frequency_penalty: Optional[float] = DEFAULT_FREQUENCY_PENALTY,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a completion using the GPT-3.5-turbo model.\n",
    "\n",
    "    messages needs to be in the format:\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, how are you?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am doing well, how about you?\"\n",
    "        }\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    if history:\n",
    "        messages = history + messages\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=CURRENT_OPENAI_LATEST_GPT_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        n=n,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "    if response is None or response[\"choices\"] is None or len(response[\"choices\"]) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    choices = response[\"choices\"]\n",
    "    top_choice = choices[0]\n",
    "    preview = top_choice[\"message\"][\"content\"].strip()\n",
    "\n",
    "    messages = messages + [{\"role\": \"assistant\", \"content\": preview}]\n",
    "    return messages, preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import sys\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "DEFAULT_MAX_TOKENS = 150\n",
    "DEFAULT_TEMPERATURE = 0.7\n",
    "DEFAULT_TOP_P = 1.0\n",
    "DEFAULT_N = 1\n",
    "DEFAULT_FREQUENCY_PENALTY = -0.5\n",
    "\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_all_links(url: str, html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    links = [link if link.startswith('http') else url + link for link in links]\n",
    "    return links\n",
    "\n",
    "def infer_jobs_url(links: List[str]) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Given the following list of URLs, identify the most likely URL for the jobs or careers page. IMPORTANT: ONLY include the link in your response:\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\\n\".join(links)[0:8000]}\n",
    "    ]\n",
    "    response = wrapped_chat_gpt_completion(messages=messages)\n",
    "    inferred_url = response[1]\n",
    "    return inferred_url\n",
    "\n",
    "def summarize_jobs_page(html: str, retries=3) -> str:\n",
    "    # only keep text from website\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text()\n",
    "        html = \" \".join(text.split())\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"List all the job titles found in this HTML in a JSON with the key 'titles' and the value as an array of job titles: {html[0:8000]}\"}\n",
    "        ]\n",
    "        response = wrapped_chat_gpt_completion(messages=messages, max_tokens=400)\n",
    "        summary = response[1]\n",
    "\n",
    "        json_test = json.loads(summary)\n",
    "\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to summarize jobs page: {e}\")\n",
    "        if retries > 0:\n",
    "            return summarize_jobs_page(html, retries - 1)\n",
    "        return \"\\{\\}\"\n",
    "\n",
    "def summarize_hiring_needs(titles: list[str]) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Given the following list of job titles, at a high level, describe what types of roles the company is hiring for in one sentence. (i.e. [company] is hiring for [roles] to [particular reason]): {titles}\"}\n",
    "    ]\n",
    "    response = wrapped_chat_gpt_completion(messages=messages, max_tokens=400)\n",
    "    summary = response[1]\n",
    "    return summary\n",
    "\n",
    "def fetch_job_data(website_url: str):\n",
    "\n",
    "    print(\"1. Analyzing website: {website_url}\\n\".format(website_url=website_url))\n",
    "    \n",
    "    html = fetch_html(website_url)\n",
    "    if not html:\n",
    "        print(\"Failed to fetch main page.\")\n",
    "        return\n",
    "    \n",
    "    links = get_all_links(website_url, html)\n",
    "    jobs_url = infer_jobs_url(links)\n",
    "    print(\"2. Found jobs URL: {jobs_url}\\n\".format(jobs_url=jobs_url))\n",
    "\n",
    "    jobs_html = fetch_html(jobs_url)\n",
    "    if not jobs_html:\n",
    "        print(\"Failed to fetch jobs page.\")\n",
    "        return\n",
    "\n",
    "    summary = summarize_jobs_page(jobs_html)\n",
    "\n",
    "    print(f\"3. Summary of active job listings: {summary}\\n\")\n",
    "\n",
    "    summary = json.loads(summary)\n",
    "\n",
    "    hiring_needs_summary = summarize_hiring_needs(summary.get('titles', []))\n",
    "    print(f\"4. Summary of hiring needs: {hiring_needs_summary}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"jobs_url\": jobs_url,\n",
    "        \"titles\": summary.get('titles', []),\n",
    "        \"hiring_needs_summary\": hiring_needs_summary,\n",
    "        \"date\": datetime.datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Analyzing website: https://www.athelas.com\n",
      "\n",
      "2. Found jobs URL: https://www.athelas.com/careers\n",
      "\n",
      "3. Summary of active job listings: {\n",
      "  \"titles\": [\n",
      "    \"Account Manager (RCM)\",\n",
      "    \"Head of Revenue Cycle Management, Orange\",\n",
      "    \"Launcher / Implementation Manager (RCM)\",\n",
      "    \"Operations Associate\",\n",
      "    \"Operations Manager\",\n",
      "    \"Operator (Billing)\",\n",
      "    \"Account Executive\",\n",
      "    \"Sales Development Representative\",\n",
      "    \"Software Engineer\",\n",
      "    \"Software Engineer Intern\",\n",
      "    \"Executive Assistant\",\n",
      "    \"Information Security and Technology Manager (InfoSec/IT)\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "4. Summary of hiring needs: The company is hiring for a wide range of roles across operations, sales, software engineering, and management, indicating a focus on expanding their business operations, enhancing their revenue management, improving their software development, and strengthening their information security affairs.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'jobs_url': 'https://www.athelas.com/careers',\n",
       " 'titles': ['Account Manager (RCM)',\n",
       "  'Head of Revenue Cycle Management, Orange',\n",
       "  'Launcher / Implementation Manager (RCM)',\n",
       "  'Operations Associate',\n",
       "  'Operations Manager',\n",
       "  'Operator (Billing)',\n",
       "  'Account Executive',\n",
       "  'Sales Development Representative',\n",
       "  'Software Engineer',\n",
       "  'Software Engineer Intern',\n",
       "  'Executive Assistant',\n",
       "  'Information Security and Technology Manager (InfoSec/IT)'],\n",
       " 'hiring_needs_summary': 'The company is hiring for a wide range of roles across operations, sales, software engineering, and management, indicating a focus on expanding their business operations, enhancing their revenue management, improving their software development, and strengthening their information security affairs.',\n",
       " 'date': '2023-10-24T11:19:44.866894'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_job_data('https://www.athelas.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.paramount.com\n",
      "Failed to scrape https://www.paramount.com\n",
      "Scraping https://mycareer.verizon.com/life-at-verizon/benefits/?cid=oso_linkedin_topic_CTA&BID=3138&utm_source=LinkedIn&utm_medium=Social&utm_content=_linkedin_CTA&utm_campaign=OrganicSocial\n",
      "Failed to scrape https://mycareer.verizon.com/life-at-verizon/benefits/?cid=oso_linkedin_topic_CTA&BID=3138&utm_source=LinkedIn&utm_medium=Social&utm_content=_linkedin_CTA&utm_campaign=OrganicSocial\n",
      "Scraping http://www.navyfederal.org\n",
      "Failed to scrape http://www.navyfederal.org\n",
      "Scraping https://www.bankofamerica.com\n",
      "Failed to scrape https://www.bankofamerica.com\n",
      "Scraping http://www.highmarkhealth.org\n",
      "Failed to scrape http://www.highmarkhealth.org\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gq/q6vwgcys2k54w_611fy1fqdm0000gn/T/ipykernel_21778/138809769.py\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mscrape_website_urls_in_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grimes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gq/q6vwgcys2k54w_611fy1fqdm0000gn/T/ipykernel_21778/138809769.py\u001b[0m in \u001b[0;36mscrape_website_urls_in_csv\u001b[0;34m(csv_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mscrape_website_urls_in_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grimes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "def scrape_website_urls_in_csv(csv_name: str):\n",
    "    fieldnames = ['linkedin_url', 'id', 'full_name', 'company', 'company_url', 'Company Job Board URL', 'Open Positions', 'Summary', 'ran_check']\n",
    "\n",
    "    with open(csv_name, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['ran_check'] == 'TRUE':\n",
    "                continue\n",
    "            company_url = row['company_url']\n",
    "            print(f\"Scraping {company_url}\")\n",
    "            try:\n",
    "                job_data = fetch_job_data(company_url)\n",
    "                if not job_data:\n",
    "                    continue\n",
    "                row['Company Job Board URL'] = job_data['jobs_url']\n",
    "                row['Open Positions'] = job_data['titles']\n",
    "                row['Summary'] = job_data['hiring_needs_summary']\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape {company_url}: {e}\")\n",
    "            row['ran_check'] = 'TRUE'\n",
    "            with open(csv_name, 'a') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow(row)\n",
    "            time.sleep(5)\n",
    "\n",
    "scrape_website_urls_in_csv('grimes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60a39ed7d9ed506b3549da2e10402e1e4204e8b41b0e183e3e35940f3cb41bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
