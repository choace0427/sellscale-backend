{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import sys\n",
    "import openai\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from src/ml/openai_wrappers.py\n",
    "from ctypes import Union\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "CURRENT_OPENAI_DAVINCI_MODEL = \"text-davinci-003\"\n",
    "CURRENT_OPENAI_CHAT_GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "CURRENT_OPENAI_LATEST_GPT_MODEL = \"gpt-4\"\n",
    "DEFAULT_SUFFIX = None\n",
    "DEFAULT_MAX_TOKENS = 16\n",
    "DEFAULT_TEMPERATURE = 1\n",
    "DEFAULT_TOP_P = 1\n",
    "DEFAULT_N = 1\n",
    "DEFAULT_FREQUENCY_PENALTY = 0\n",
    "DEFAULT_STOP = None\n",
    "\n",
    "def wrapped_chat_gpt_completion(\n",
    "    messages: list,\n",
    "    history: Optional[list] = [],\n",
    "    max_tokens: Optional[int] = DEFAULT_MAX_TOKENS,\n",
    "    temperature: Optional[float] = DEFAULT_TEMPERATURE,\n",
    "    top_p: Optional[float] = DEFAULT_TOP_P,\n",
    "    n: Optional[int] = DEFAULT_N,\n",
    "    frequency_penalty: Optional[float] = DEFAULT_FREQUENCY_PENALTY,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a completion using the GPT-3.5-turbo model.\n",
    "\n",
    "    messages needs to be in the format:\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, how are you?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am doing well, how about you?\"\n",
    "        }\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    if history:\n",
    "        messages = history + messages\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=CURRENT_OPENAI_LATEST_GPT_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        n=n,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "    if response is None or response[\"choices\"] is None or len(response[\"choices\"]) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    choices = response[\"choices\"]\n",
    "    top_choice = choices[0]\n",
    "    preview = top_choice[\"message\"][\"content\"].strip()\n",
    "\n",
    "    messages = messages + [{\"role\": \"assistant\", \"content\": preview}]\n",
    "    return messages, preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import sys\n",
    "import openai\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "DEFAULT_MAX_TOKENS = 150\n",
    "DEFAULT_TEMPERATURE = 0.7\n",
    "DEFAULT_TOP_P = 1.0\n",
    "DEFAULT_N = 1\n",
    "DEFAULT_FREQUENCY_PENALTY = -0.5\n",
    "\n",
    "\n",
    "def fetch_html(url: str) -> str:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_all_links(url: str, html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    links = [link if link.startswith('http') else url + link for link in links]\n",
    "    return links\n",
    "\n",
    "def infer_jobs_url(links: List[str]) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Given the following list of URLs, identify the most likely URL for the jobs or careers page. IMPORTANT: ONLY include the link in your response:\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\\n\".join(links)[0:8000]}\n",
    "    ]\n",
    "    response = wrapped_chat_gpt_completion(messages=messages)\n",
    "    inferred_url = response[1]\n",
    "    return inferred_url\n",
    "\n",
    "def summarize_jobs_page(html: str, retries=3) -> str:\n",
    "    # only keep text from website\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text()\n",
    "        html = \" \".join(text.split())\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"List all the job titles found in this HTML in a JSON with the key 'titles' and the value as an array of job titles: {html[0:8000]}\"}\n",
    "        ]\n",
    "        response = wrapped_chat_gpt_completion(messages=messages, max_tokens=400)\n",
    "        summary = response[1]\n",
    "\n",
    "        json_test = json.loads(summary)\n",
    "\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to summarize jobs page: {e}\")\n",
    "        if retries > 0:\n",
    "            return summarize_jobs_page(html, retries - 1)\n",
    "        return \"\\{\\}\"\n",
    "\n",
    "def summarize_hiring_needs(titles: list[str]) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Given the following list of job titles, at a high level, describe what types of roles the company is hiring for in one sentence. (i.e. [company] is hiring for [roles] to [particular reason]): {titles}\"}\n",
    "    ]\n",
    "    response = wrapped_chat_gpt_completion(messages=messages, max_tokens=400)\n",
    "    summary = response[1]\n",
    "    return summary\n",
    "\n",
    "def fetch_job_data(website_url: str):\n",
    "\n",
    "    print(\"1. Analyzing website: {website_url}\\n\".format(website_url=website_url))\n",
    "    \n",
    "    html = fetch_html(website_url)\n",
    "    if not html:\n",
    "        print(\"Failed to fetch main page.\")\n",
    "        return\n",
    "    \n",
    "    links = get_all_links(website_url, html)\n",
    "    jobs_url = infer_jobs_url(links)\n",
    "    print(\"2. Found jobs URL: {jobs_url}\\n\".format(jobs_url=jobs_url))\n",
    "\n",
    "    jobs_html = fetch_html(jobs_url)\n",
    "    if not jobs_html:\n",
    "        print(\"Failed to fetch jobs page.\")\n",
    "        return\n",
    "\n",
    "    summary = summarize_jobs_page(jobs_html)\n",
    "\n",
    "    print(f\"3. Summary of active job listings: {summary}\\n\")\n",
    "\n",
    "    summary = json.loads(summary)\n",
    "\n",
    "    hiring_needs_summary = summarize_hiring_needs(summary.get('titles', []))\n",
    "    print(f\"4. Summary of hiring needs: {hiring_needs_summary}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"jobs_url\": jobs_url,\n",
    "        \"titles\": summary.get('titles', []),\n",
    "        \"hiring_needs_summary\": hiring_needs_summary,\n",
    "        \"date\": datetime.datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Analyzing website: https://www.sellscale.com\n",
      "\n",
      "2. Found jobs URL: https://www.sellscale.com/careers\n",
      "\n",
      "3. Summary of active job listings: {\n",
      "  \"titles\": [\n",
      "    \"Founding Software Engineer\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "4. Summary of hiring needs: The company is hiring for a Founding Software Engineer role, to possibly take the lead in building the foundation of the company's tech stack and driving initial product development.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'jobs_url': 'https://www.sellscale.com/careers',\n",
       " 'titles': ['Founding Software Engineer'],\n",
       " 'hiring_needs_summary': \"The company is hiring for a Founding Software Engineer role, to possibly take the lead in building the foundation of the company's tech stack and driving initial product development.\",\n",
       " 'date': '2023-10-24T20:30:10.883745'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_job_data('https://www.sellscale.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "def scrape_website_urls_in_csv(csv_name: str):\n",
    "    fieldnames = ['linkedin_url', 'id', 'full_name', 'company', 'company_url', 'Company Job Board URL', 'Open Positions', 'Summary', 'ran_check']\n",
    "\n",
    "    with open(csv_name, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['ran_check'] == 'TRUE':\n",
    "                continue\n",
    "            company_url = row['company_url']\n",
    "            print(f\"Scraping {company_url}\")\n",
    "            try:\n",
    "                job_data = fetch_job_data(company_url)\n",
    "                if not job_data:\n",
    "                    continue\n",
    "                row['Company Job Board URL'] = job_data['jobs_url']\n",
    "                row['Open Positions'] = job_data['titles']\n",
    "                row['Summary'] = job_data['hiring_needs_summary']\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape {company_url}: {e}\")\n",
    "            row['ran_check'] = 'TRUE'\n",
    "            with open(csv_name, 'a') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writerow(row)\n",
    "            time.sleep(5)\n",
    "\n",
    "scrape_website_urls_in_csv('grimes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this query:\n",
    "```\n",
    "select \n",
    "\tlinkedin_url,\n",
    "\tid,\n",
    "\tfull_name,\n",
    "\tcompany,\n",
    "\tcompany_url,\n",
    "\ttitle,\n",
    "\t'' \"Company Job Board URL\",\n",
    "\t'' \"Open Positions\",\n",
    "\t'' \"Summary\",\n",
    "\t'' \"ran_check\"\n",
    "\t\n",
    "from prospect\n",
    "where client_sdr_id = 103\n",
    "\tand prospect.overall_status = 'PROSPECTED'\n",
    "\tand prospect.icp_fit_score > 2\n",
    "\tand company_url is not null\n",
    "\tand prospect.industry is not null\n",
    "order by employee_count asc\n",
    "limit 300;\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60a39ed7d9ed506b3549da2e10402e1e4204e8b41b0e183e3e35940f3cb41bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
