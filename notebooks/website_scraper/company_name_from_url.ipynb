{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name_from_opengraph(urls: list[str]) -> list[str]:\n",
    "    \n",
    "    company_names = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Fetch the HTML content of the webpage\n",
    "            response = requests.get(url, timeout=2)\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find Open Graph meta tags\n",
    "            og_title = soup.find('meta', property='og:site_name')\n",
    "            if og_title:\n",
    "                company_names.append(og_title['content'])\n",
    "                print(og_title['content'], url)\n",
    "            else:\n",
    "                print(\"No Open Graph meta tags found for the URL:\", url)\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "    return company_names\n",
    "\n",
    "def get_company_name_from_html_title_tag(urls: list[str]) -> list[str]:\n",
    "    \n",
    "    company_names = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Fetch the HTML content of the webpage\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the title tag\n",
    "            title_tag = soup.find('title').text.strip()\n",
    "            if title_tag:\n",
    "                company_names.append(title_tag)\n",
    "                print(title_tag, url)\n",
    "            else:\n",
    "                print(\"No title tag found for the URL:\", url)\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "    return company_names\n",
    "\n",
    "def get_domain_name_using_regex(urls: list[str]) -> list[str]:\n",
    "\n",
    "    company_names = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Use regex to extract the domain name\n",
    "            domain = re.search(r\"(?:https?://)?(?:www\\.)?([^./]+(?:\\.[^./]+)+)\", url).group(1)\n",
    "\n",
    "            # Make the domain name title case\n",
    "            domain = domain.title()\n",
    "            company_names.append(domain)\n",
    "            print(domain, url)\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "    return company_names\n",
    "\n",
    "\n",
    "def get_domain_name_using_urllib(urls: list[str]) -> list[str]:\n",
    "    from urllib.parse import urlparse\n",
    "\n",
    "    company_names = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Parse the URL\n",
    "            parsed_url = urlparse(url)\n",
    "            # Remove 'www.' if present\n",
    "            netloc = parsed_url.netloc.replace(\"www.\", \"\")\n",
    "            # Extract the domain name\n",
    "            domain_parts = netloc.split('.')\n",
    "            if len(domain_parts) > 2:\n",
    "                domain_name = '.'.join(domain_parts[-2:])\n",
    "            else:\n",
    "                domain_name = netloc\n",
    "\n",
    "            domain_name = domain_name.title()\n",
    "            domain_name = domain_name.split('.')[0]\n",
    "            company_names.append(domain_name)\n",
    "            print(domain_name, url)\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "        \n",
    "    return company_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tinybeans https://www.tinybeans.com/\n",
      "Barkbox https://barkbox.com/\n",
      "Warbyparker https://www.warbyparker.com/\n",
      "Awaytravel https://www.awaytravel.com/\n",
      "Glossier https://www.glossier.com/\n",
      "Amazon https://www.amazon.com/\n",
      "Google https://www.google.com/\n",
      "Apple https://www.apple.com/\n",
      "Microsoft https://www.microsoft.com/\n",
      "Meta https://www.meta.com/\n",
      "Meundies https://www.meundies.com/\n",
      "Casper https://casper.com/\n",
      "Allbirds https://www.allbirds.com/\n",
      "Chubbiesshorts https://www.chubbiesshorts.com/\n",
      "Bird https://www.bird.co/\n",
      "Tesla https://www.tesla.com/\n",
      "Netflix https://www.netflix.com/\n",
      "Coca-Colacompany https://www.coca-colacompany.com/\n",
      "Walmart https://www.walmart.com/\n",
      "Nike https://www.nike.com/\n",
      "Canva https://www.canva.com/\n",
      "Figma https://www.figma.com/\n",
      "Grammarly https://www.grammarly.com/\n",
      "Notion https://www.notion.so/\n",
      "Trello https://trello.com/\n",
      "Disney https://www.disney.com/\n",
      "Ibm https://www.ibm.com/\n",
      "Mcdonalds https://www.mcdonalds.com/\n",
      "Starbucks https://www.starbucks.com/\n",
      "Airbnb https://www.airbnb.com/\n",
      "Northmarq https://www.welcome.northmarq.com/about/team\n"
     ]
    }
   ],
   "source": [
    "company_urls = [\n",
    "    \"https://www.tinybeans.com/\",\n",
    "    \"https://barkbox.com/\",\n",
    "    \"https://www.warbyparker.com/\",\n",
    "    \"https://www.awaytravel.com/\",\n",
    "    \"https://www.glossier.com/\",\n",
    "    \"https://www.amazon.com/\",\n",
    "    \"https://www.google.com/\",\n",
    "    \"https://www.apple.com/\",\n",
    "    \"https://www.microsoft.com/\",\n",
    "    \"https://www.meta.com/\",\n",
    "    \"https://www.meundies.com/\",\n",
    "    \"https://casper.com/\",\n",
    "    \"https://www.allbirds.com/\",\n",
    "    \"https://www.chubbiesshorts.com/\",\n",
    "    \"https://www.bird.co/\",\n",
    "    \"https://www.tesla.com/\",\n",
    "    \"https://www.netflix.com/\",\n",
    "    \"https://www.coca-colacompany.com/\",\n",
    "    \"https://www.walmart.com/\",\n",
    "    \"https://www.nike.com/\",\n",
    "    \"https://www.canva.com/\",\n",
    "    \"https://www.figma.com/\",\n",
    "    \"https://www.grammarly.com/\",\n",
    "    \"https://www.notion.so/\",\n",
    "    \"https://trello.com/\",\n",
    "    \"https://www.disney.com/\",\n",
    "    \"https://www.ibm.com/\",\n",
    "    \"https://www.mcdonalds.com/\",\n",
    "    \"https://www.starbucks.com/\",\n",
    "    \"https://www.airbnb.com/\",\n",
    "    \"https://www.welcome.northmarq.com/about/team\"\n",
    "]\n",
    "\n",
    "# company_names = get_company_name_from_opengraph(urls=company_urls)\n",
    "# company_names = get_company_name_from_html_title_tag(urls=company_urls)\n",
    "# company_names = get_domain_name_using_regex(urls=company_urls)\n",
    "company_names = get_domain_name_using_urllib(urls=company_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
