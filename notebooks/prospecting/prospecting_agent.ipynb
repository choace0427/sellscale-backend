{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI HELPER FUNCTIONS\n",
    "from ast import Dict, List\n",
    "from ctypes import Union\n",
    "import re\n",
    "from typing import Any, Dict, List, NewType, Optional, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "\n",
    "from serpapi import GoogleSearch\n",
    "import openai\n",
    "\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "# map src\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.utils.slack import send_slack_message, URL_MAP\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "serp_api_key = os.environ.get(\"SERP_API_KEY\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "env_path = Path(\"../..\") / \".envprod\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "ISCRAPER_API_KEY = os.environ.get(\"ISCRAPER_API_KEY\")\n",
    "PROFILE_DETAILS_URL = \"https://api.proapis.com/iscraper/v4/profile-details\"\n",
    "\n",
    "\n",
    "CURRENT_OPENAI_DAVINCI_MODEL = \"text-davinci-003\"\n",
    "CURRENT_OPENAI_CHAT_GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "CURRENT_OPENAI_LATEST_GPT_MODEL = \"gpt-4\"\n",
    "DEFAULT_SUFFIX = None\n",
    "DEFAULT_MAX_TOKENS = 16\n",
    "DEFAULT_TEMPERATURE = 1\n",
    "DEFAULT_TOP_P = 1\n",
    "DEFAULT_N = 1\n",
    "DEFAULT_FREQUENCY_PENALTY = 0\n",
    "DEFAULT_STOP = None\n",
    "\n",
    "NUM_GOOGLE_RESULTS_TO_SCRAPE = 10\n",
    "MAX_NUM_PROFILES_TO_PROCESS = 100\n",
    "\n",
    "API_URL = \"https://sellscale-api-prod.onrender.com\"\n",
    "\n",
    "\n",
    "\n",
    "def search_google_news(query, type: Optional[str] = None):\n",
    "    # https://support.google.com/websearch/answer/2466433?hl=en\n",
    "    params = {\n",
    "        \"api_key\": serp_api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"tbm\": type,\n",
    "        \"gl\": \"us\",  # US only\n",
    "        \"hl\": \"en\",\n",
    "        \"num\": NUM_GOOGLE_RESULTS_TO_SCRAPE,\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def wrapped_chat_gpt_completion(\n",
    "    messages: list,\n",
    "    history: Optional[list] = [],\n",
    "    max_tokens: Optional[int] = DEFAULT_MAX_TOKENS,\n",
    "    temperature: Optional[float] = DEFAULT_TEMPERATURE,\n",
    "    top_p: Optional[float] = DEFAULT_TOP_P,\n",
    "    n: Optional[int] = DEFAULT_N,\n",
    "    frequency_penalty: Optional[float] = DEFAULT_FREQUENCY_PENALTY,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a completion using the GPT-3.5-turbo model.\n",
    "\n",
    "    messages needs to be in the format:\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, how are you?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am doing well, how about you?\"\n",
    "        }\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    if history:\n",
    "        messages = history + messages\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=CURRENT_OPENAI_LATEST_GPT_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        n=n,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "    if response is None or response[\"choices\"] is None or len(response[\"choices\"]) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    choices = response[\"choices\"]\n",
    "    top_choice = choices[0]\n",
    "    preview = top_choice[\"message\"][\"content\"].strip()\n",
    "\n",
    "    messages = messages + [{\"role\": \"assistant\", \"content\": preview}]\n",
    "    return messages, preview\n",
    "\n",
    "\n",
    "def call_iscraper(linkedin_id: str):\n",
    "    payload = json.dumps(\n",
    "        {\n",
    "            \"profile_id\": linkedin_id,\n",
    "            \"profile_type\": \"personal\",\n",
    "            \"network_info\": True,\n",
    "        }\n",
    "    )\n",
    "    headers = {\"X-API-KEY\": ISCRAPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", PROFILE_DETAILS_URL, headers=headers, data=payload\n",
    "    )\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def research_corporate_profile_details(company_name: str):\n",
    "    payload = json.dumps(\n",
    "        {\n",
    "            \"profile_id\": company_name,\n",
    "            \"profile_type\": \"company\",\n",
    "            \"contact_info\": True,\n",
    "            \"recommendations\": True,\n",
    "            \"related_profiles\": True,\n",
    "            \"network_info\": True,\n",
    "        }\n",
    "    )\n",
    "    headers = {\"X-API-KEY\": ISCRAPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", PROFILE_DETAILS_URL, headers=headers, data=payload\n",
    "    )\n",
    "\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def str_path_to_path_steps(path: str, *delimiters: str):\n",
    "    steps = re.split(\"|\".join(map(re.escape, delimiters)), path)\n",
    "    for step in steps:\n",
    "        if step.isdigit():\n",
    "            yield int(step)\n",
    "        else:\n",
    "            yield step\n",
    "\n",
    "\n",
    "def deep_get(obj: Union[List, Dict], path: str, default=None) -> Optional[Any]:\n",
    "    steps = str_path_to_path_steps(path, \".\")\n",
    "\n",
    "    for step in steps:\n",
    "        if not obj:\n",
    "            return default\n",
    "\n",
    "        if isinstance(obj, dict):\n",
    "            obj = obj.get(step, None)\n",
    "        elif isinstance(obj, list) and str(step).isnumeric():\n",
    "            obj = obj[int(step)]\n",
    "        else:\n",
    "            try:\n",
    "                obj = getattr(obj, step)\n",
    "            except:\n",
    "                return default\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_company_info(event_query: str):\n",
    "    # Fetch recent news articles related to the event\n",
    "    news_results = search_google_news(event_query, \"nws\")\n",
    "\n",
    "    # Prepare data for CSV\n",
    "    csv_data = []\n",
    "\n",
    "    for article in news_results.get(\"news_results\", []):\n",
    "        # Prepare the message for Chat GPT to extract the company name\n",
    "        chat_message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Which company does this news article pertain to? {article['title']} {article['snippet']}\\nOnly respond with the company name. If company not found, return 'none' all lowercase.\\nCompany name:\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Extract company name using Chat GPT\n",
    "        _, company_name = wrapped_chat_gpt_completion(chat_message)\n",
    "\n",
    "        # If company name is not found, skip the article\n",
    "        if \"none\" in company_name.lower():\n",
    "            continue\n",
    "\n",
    "        # Append the details to the CSV data list\n",
    "        csv_data.append(\n",
    "            {\n",
    "                \"img_url\": article.get(\"thumbnail\"),\n",
    "                \"title\": article.get(\"title\"),\n",
    "                \"snippet\": article.get(\"snippet\"),\n",
    "                \"link\": article.get(\"link\"),\n",
    "                \"date\": article.get(\"date\"),\n",
    "                \"company_name\": company_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Generate a timestamped CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # file name with timestamp, and the query\n",
    "    csv_filename = f\"{timestamp}-{event_query}.csv\"\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(\n",
    "            file,\n",
    "            fieldnames=[\"img_url\", \"title\", \"snippet\", \"link\", \"date\", \"company_name\"],\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_data)\n",
    "\n",
    "    print(f\"Data extraction completed. CSV file created: {csv_filename}\")\n",
    "\n",
    "    return csv_filename\n",
    "\n",
    "\n",
    "def qualify_company_events(csv_filename: str, ai_qualifying_question: str):\n",
    "    # Read the existing CSV file\n",
    "    with open(csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Process each row to add 'qualified' column\n",
    "    for row in data:\n",
    "        # Prepare the message for Chat GPT\n",
    "        chat_message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{ai_qualifying_question} Event: {row['title']}. Company: {row['company_name']}.\\nOnly respond with 'true' or 'false'.\\nResponse:\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Get qualification response using Chat GPT\n",
    "        _, qualification_response = wrapped_chat_gpt_completion(chat_message)\n",
    "\n",
    "        # Determine qualification (True/False)\n",
    "        qualified = \"true\" in qualification_response.lower()\n",
    "\n",
    "        # Add 'qualified' field to the row\n",
    "        row[\"qualified\"] = qualified\n",
    "\n",
    "    # Write the updated data to a new CSV file\n",
    "    new_csv_filename = csv_filename.replace(\".csv\", \"-qualified.csv\")\n",
    "    with open(new_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Qualification completed. Updated CSV file created: {new_csv_filename}\")\n",
    "\n",
    "    return new_csv_filename\n",
    "\n",
    "\n",
    "def extract_linkedin_profiles(qualified_csv_filename: str, titles: list):\n",
    "    # Read the qualified companies\n",
    "    with open(qualified_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        qualified_companies = [\n",
    "            row for row in reader if row[\"qualified\"].lower() == \"true\"\n",
    "        ]\n",
    "\n",
    "    # Prepare data for CSV\n",
    "    profiles_data = []\n",
    "\n",
    "    # Loop through each company and title\n",
    "    for company in qualified_companies:\n",
    "        for title in titles:\n",
    "            # Construct the Google search query\n",
    "            query = f'site:linkedin.com/in/ \"{company[\"company_name\"]}\" \"- {title}\"'\n",
    "\n",
    "            # Perform the Google search\n",
    "            search_results = search_google_news(\n",
    "                query\n",
    "            )  # Use your search_google_news function\n",
    "            organic_results = search_results.get(\"organic_results\", [])\n",
    "\n",
    "            # Process search results\n",
    "            for profile in organic_results:\n",
    "                # Extract relevant profile details\n",
    "                profile_data = {\n",
    "                    \"img_url\": company.get(\"img_url\"),  # Original data\n",
    "                    \"original_title\": company.get(\"title\"),  # Original data\n",
    "                    \"snippet\": company.get(\"snippet\"),  # Original data\n",
    "                    \"original_link\": company.get(\"link\"),  # Original data\n",
    "                    \"date\": company.get(\"date\"),  # Original data\n",
    "                    \"company_name\": company[\"company_name\"],  # Original data\n",
    "                    \"linkedin_title\": title,  # LinkedIn profile title\n",
    "                    \"profile_url\": profile.get(\"link\"),  # LinkedIn profile URL\n",
    "                }\n",
    "                profiles_data.append(profile_data)\n",
    "\n",
    "    # Generate a CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    csv_filename = f\"{timestamp}-linkedin-profiles.csv\"\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=profiles_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(profiles_data)\n",
    "\n",
    "    print(f\"LinkedIn profiles extraction completed. CSV file created: {csv_filename}\")\n",
    "\n",
    "    return csv_filename\n",
    "\n",
    "\n",
    "def enrich_linkedin_profiles(linkedin_csv_filename: str):\n",
    "    # Read the LinkedIn profiles CSV\n",
    "    with open(linkedin_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        linkedin_data = list(reader)\n",
    "\n",
    "    enriched_data = []\n",
    "\n",
    "    for row in tqdm(linkedin_data[0:MAX_NUM_PROFILES_TO_PROCESS]):\n",
    "        # Call the iScraper API\n",
    "        profile_url = row[\"profile_url\"]\n",
    "        profile_id = profile_url.split(\"/in/\")[1]\n",
    "        iscraper_response = call_iscraper(\n",
    "            profile_id\n",
    "        )  # Assuming this function is already defined\n",
    "        time.sleep(1)  # Wait for 1 second between API calls\n",
    "\n",
    "        # Extract required fields from the iScraper response\n",
    "        first_name = iscraper_response.get(\"first_name\", \"\")\n",
    "        last_name = iscraper_response.get(\"last_name\", \"\")\n",
    "        company = deep_get(\n",
    "            iscraper_response,\n",
    "            \"position_groups.0.profile_positions.0.company\",\n",
    "            default=\"\",\n",
    "        )\n",
    "        sub_title = iscraper_response.get(\"sub_title\", \"\")\n",
    "        summary = iscraper_response.get(\"summary\", \"\")\n",
    "        title = deep_get(\n",
    "            iscraper_response, \"position_groups.0.profile_positions.0.title\", default=\"\"\n",
    "        )\n",
    "        industry = iscraper_response.get(\"industry\", \"\")\n",
    "        profile_picture = iscraper_response.get(\"profile_picture\", \"\")\n",
    "        raw_json = json.dumps(iscraper_response)\n",
    "\n",
    "        # Prepare the enriched row\n",
    "        enriched_row = {\n",
    "            **row,\n",
    "            \"prospect_first_name\": first_name,\n",
    "            \"prospect_last_name\": last_name,\n",
    "            \"prospect_company\": company,\n",
    "            \"prospect_sub_title\": sub_title,\n",
    "            \"prospect_summary\": summary,\n",
    "            \"prospect_title\": title,\n",
    "            \"prospect_industry\": industry,\n",
    "            \"profile_picture\": profile_picture,\n",
    "            \"raw_iscraper_json\": raw_json,\n",
    "        }\n",
    "\n",
    "        enriched_data.append(enriched_row)\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    enriched_csv_filename = f\"{timestamp}-enriched-linkedin-profiles.csv\"\n",
    "\n",
    "    # Write the data to the new CSV file\n",
    "    with open(enriched_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=enriched_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(enriched_data)\n",
    "\n",
    "    print(\n",
    "        f\"LinkedIn profiles enrichment completed. Enriched CSV file created: {enriched_csv_filename}\"\n",
    "    )\n",
    "\n",
    "    return enriched_csv_filename\n",
    "\n",
    "\n",
    "def perform_gpt_checks_on_profiles(enriched_csv_filename: str, roles: list):\n",
    "    # Read the enriched LinkedIn profiles CSV\n",
    "    with open(enriched_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        enriched_data = list(reader)\n",
    "\n",
    "    checked_data = []\n",
    "\n",
    "    print(\"Performing GPT checks on LinkedIn profiles...\")\n",
    "\n",
    "    for row in tqdm(enriched_data):\n",
    "        # Check if the company matches (case-insensitive)\n",
    "        correct_company = (\n",
    "            row[\"company_name\"].lower() == row[\"prospect_company\"].lower()\n",
    "            or row[\"company_name\"].lower() in row[\"prospect_company\"].lower()\n",
    "            or row[\"prospect_company\"].lower() in row[\"company_name\"].lower()\n",
    "        )\n",
    "        row[\"correct_company\"] = correct_company\n",
    "\n",
    "        # Prepare the message for Chat GPT to check the role\n",
    "        chat_message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Does the role '{row['prospect_title']}' match any of these roles: {roles}?\\nOnly respond with 'true' or 'false'.\\nResponse:\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Get the role match response using Chat GPT\n",
    "        _, role_match_response = wrapped_chat_gpt_completion(chat_message)\n",
    "\n",
    "        # Determine if the role matches (True/False)\n",
    "        correct_role = \"true\" in role_match_response.lower()\n",
    "        row[\"correct_role\"] = correct_role\n",
    "\n",
    "        checked_data.append(row)\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    checked_csv_filename = f\"{timestamp}-checked-linkedin-profiles.csv\"\n",
    "\n",
    "    # Write the data to the new CSV file\n",
    "    with open(checked_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=checked_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(checked_data)\n",
    "\n",
    "    print(\n",
    "        f\"LinkedIn profiles checking completed. Checked CSV file created: {checked_csv_filename}\"\n",
    "    )\n",
    "\n",
    "    return checked_csv_filename\n",
    "\n",
    "\n",
    "def create_final_filtered_csv(checked_csv_filename: str):\n",
    "    # Read the checked LinkedIn profiles CSV\n",
    "    with open(checked_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        checked_data = list(reader)\n",
    "\n",
    "    # Filter data where both correct_role and correct_company are True\n",
    "    filtered_data = [\n",
    "        row\n",
    "        for row in checked_data\n",
    "        if row[\"correct_role\"].lower() == \"true\"\n",
    "        and row[\"correct_company\"].lower() == \"true\"\n",
    "    ]\n",
    "\n",
    "    # remove duplicates on linkedin url\n",
    "    filtered_data = [dict(t) for t in {tuple(d.items()) for d in filtered_data}]\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    final_csv_filename = f\"{timestamp}-final-filtered-profiles.csv\"\n",
    "\n",
    "    # Write the filtered data to the new CSV file\n",
    "    with open(final_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=filtered_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    print(f\"Final filtered CSV file created: {final_csv_filename}\")\n",
    "\n",
    "    return final_csv_filename\n",
    "\n",
    "\n",
    "def send_slack_notification(\n",
    "    final_csv_filename: str,\n",
    "    trigger_name: str,\n",
    "    client_archetype_id: int,\n",
    "    client_webhook_url: str,\n",
    "):\n",
    "    # Read data from the final CSV\n",
    "    with open(final_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Prepare sample events for the Slack message\n",
    "    sample_events = {}\n",
    "    for row in data:\n",
    "        key = row[\"original_link\"]\n",
    "        if key in sample_events:\n",
    "            sample_events[key][\"prospects\"].append(\n",
    "                {\n",
    "                    \"name\": f\"{row['prospect_first_name']} {row['prospect_last_name']}\",\n",
    "                    \"linkedin_url\": row[\"profile_url\"],\n",
    "                    \"prospect_title\": row[\"prospect_title\"],\n",
    "                    \"location\": row.get(\"location\", \"\"),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            sample_events[key] = {\n",
    "                \"title\": row[\"original_title\"],\n",
    "                \"company\": row[\"company_name\"],\n",
    "                \"snippet\": row[\"snippet\"],\n",
    "                \"url\": row[\"original_link\"],\n",
    "                \"date\": row[\"date\"],\n",
    "                \"industry\": row[\"prospect_industry\"],\n",
    "                \"location\": \"United States\",\n",
    "                \"prospects\": [\n",
    "                    {\n",
    "                        \"name\": f\"{row['prospect_first_name']} {row['prospect_last_name']}\",\n",
    "                        \"linkedin_url\": row[\"profile_url\"],\n",
    "                        \"prospect_title\": row[\"prospect_title\"],\n",
    "                        \"location\": row.get(\"location\", \"\"),\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "\n",
    "    # sort events by most prospects\n",
    "    sample_events = {\n",
    "        k: v\n",
    "        for k, v in sorted(\n",
    "            sample_events.items(),\n",
    "            key=lambda item: len(item[1][\"prospects\"]),\n",
    "            reverse=True,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Send Slack message\n",
    "    print(\"Sending Slack message...\")\n",
    "    count = 0\n",
    "    for key, event in sample_events.items():\n",
    "        count += 1\n",
    "        if count > 3:\n",
    "            break\n",
    "        prospects_details = \"\\n\".join(\n",
    "            [\n",
    "                f\"> - <{prospect['linkedin_url']}|*{prospect['name']}*> - {prospect['prospect_title']}\"\n",
    "                for prospect in event[\"prospects\"][0:3]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        blocks = [\n",
    "            {\n",
    "                \"type\": \"header\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"plain_text\",\n",
    "                    \"text\": f\"Trigger ‚ö°Ô∏è: {trigger_name}\",\n",
    "                    \"emoji\": True,\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": \"\"\"> :newspaper: *<{url}|'{title}'>*\\n> {snippet}\\n> _- {date}_\"\"\".format(\n",
    "                        url=event[\"url\"],\n",
    "                        title=event[\"title\"],\n",
    "                        snippet=event[\"snippet\"].replace(\"\\n\", \"\\n> \"),\n",
    "                        date=event[\"date\"],\n",
    "                    ),\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": f\"*Company:* {event['company']}\",\n",
    "                },\n",
    "            },\n",
    "            {\"type\": \"divider\"},\n",
    "            {\n",
    "                # context\n",
    "                \"type\": \"context\",\n",
    "                \"elements\": [\n",
    "                    {\n",
    "                        \"type\": \"mrkdwn\",\n",
    "                        \"text\": f\":white_check_mark: Location: US, :white_check_mark: Industry: {event['industry']}\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": f\"*{len(event['prospects'])} prospects found*\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\"type\": \"mrkdwn\", \"text\": prospects_details},\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"divider\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        result = send_slack_message(\n",
    "            message=\"hello\",\n",
    "            webhook_urls=[URL_MAP[\"eng-sandbox\"]]\n",
    "            + ([client_webhook_url] if client_webhook_url else []),\n",
    "            blocks=blocks,\n",
    "        )\n",
    "\n",
    "        print(\"Sent Slack message for event: \" + event[\"title\"])\n",
    "\n",
    "\n",
    "def generate_upload_to_campaign_csv(final_csv_filename: str):\n",
    "    # Read data from the final CSV\n",
    "    with open(final_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Prepare data for the upload_to_campaign CSV\n",
    "    campaign_data = []\n",
    "    for row in data:\n",
    "        snippet = row[\"snippet\"].replace(\"\\n\", \" \")\n",
    "        campaign_row = {\n",
    "            \"first_name\": row[\"prospect_first_name\"],\n",
    "            \"last_name\": row[\"prospect_last_name\"],\n",
    "            \"linkedin_url\": row[\"profile_url\"],\n",
    "            \"company\": row[\"company_name\"],\n",
    "            \"title\": row[\"prospect_title\"],\n",
    "            \"custom_data\": f\"{row['company_name']} was recently in the news titled '{row['original_title']}' posted {row['date']}. The article summary is: '{snippet}'\",\n",
    "        }\n",
    "        campaign_data.append(campaign_row)\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    campaign_csv_filename = f\"{timestamp}-upload-to-campaign.csv\"\n",
    "\n",
    "    # Write the campaign data to the new CSV file\n",
    "    with open(campaign_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=campaign_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(campaign_data)\n",
    "\n",
    "    print(f\"Upload to Campaign CSV file created: {campaign_csv_filename}\")\n",
    "\n",
    "    return campaign_csv_filename\n",
    "\n",
    "\n",
    "def get_trigger_inputs(trigger_id: int, bearer_token) -> dict:\n",
    "    url = API_URL + \"/triggers/trigger/\" + str(trigger_id)\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\"Authorization\": \"Bearer \" + bearer_token}\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    return json.loads(response.text)[\"trigger_config\"]\n",
    "\n",
    "\n",
    "def run_one_trigger(trigger_id: int, bearer_token: str) -> int:\n",
    "    import requests\n",
    "\n",
    "    url = API_URL + \"/triggers/trigger/run/\" + str(trigger_id)\n",
    "\n",
    "    payload = {}\n",
    "    headers = {\"Authorization\": \"Bearer \" + bearer_token}\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    return json.loads(response.text)[\"trigger_run_id\"]\n",
    "\n",
    "\n",
    "class TriggerProspectEntry:\n",
    "    def __init__(\n",
    "        self,\n",
    "        first_name: str,\n",
    "        last_name: str,\n",
    "        linkedin_url: str,\n",
    "        company: str,\n",
    "        title: str,\n",
    "        custom_data: str,\n",
    "    ):\n",
    "        self.first_name = first_name\n",
    "        self.last_name = last_name\n",
    "        self.linkedin_url = linkedin_url\n",
    "        self.company = company\n",
    "        self.title = title\n",
    "        self.custom_data = custom_data\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"first_name\": self.first_name,\n",
    "            \"last_name\": self.last_name,\n",
    "            \"linkedin_url\": self.linkedin_url,\n",
    "            \"company\": self.company,\n",
    "            \"title\": self.title,\n",
    "            \"custom_data\": self.custom_data,\n",
    "        }\n",
    "\n",
    "\n",
    "def upload_prospects_to_campaign(\n",
    "    trigger_id: int, bearer_token: str, prospects: list[TriggerProspectEntry]\n",
    "):\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    url = API_URL + \"/triggers/trigger/run/prospects/\" + str(trigger_id)\n",
    "\n",
    "    payload = json.dumps({\"prospects\": [p.to_dict() for p in prospects]})\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \" + bearer_token,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def upload_prospects_from_csv_to_campaign(\n",
    "    trigger_id: int, bearer_token: str, csv_filename: str\n",
    "):\n",
    "    prospects = []\n",
    "    with open(csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            prospects.append(\n",
    "                TriggerProspectEntry(\n",
    "                    first_name=row[\"first_name\"],\n",
    "                    last_name=row[\"last_name\"],\n",
    "                    linkedin_url=row[\"linkedin_url\"],\n",
    "                    company=row[\"company\"],\n",
    "                    title=row[\"title\"],\n",
    "                    custom_data=row[\"custom_data\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return upload_prospects_to_campaign(trigger_id, bearer_token, prospects)\n",
    "\n",
    "\n",
    "def run_trigger(trigger_id: int, bearer_token: str):\n",
    "    inputs = get_trigger_inputs(trigger_id, bearer_token)\n",
    "    news_event_query = inputs[\"news_event_query\"]\n",
    "    ai_company_qualifying_question = inputs[\"ai_company_qualifying_question\"]\n",
    "    linkedin_titles = inputs[\"linkedin_titles\"]\n",
    "    trigger_name = inputs[\"trigger_name\"]\n",
    "    client_archetype_id = inputs[\"client_archetype_id\"]\n",
    "    client_webhook_url = inputs[\"client_webhook_url\"]\n",
    "\n",
    "    print(\"Running trigger: \" + trigger_name)\n",
    "    print(json.dumps(inputs, indent=4))\n",
    "\n",
    "    # Create Trigger Run Object\n",
    "    trigger_run_id = run_one_trigger(trigger_id, bearer_token)\n",
    "\n",
    "    # OUTPUTS\n",
    "    print(\"\\n\\n############\\nüì∞ Extracting company news events...\\n############\")\n",
    "    raw_company_events_filename = extract_event_company_info(news_event_query)\n",
    "    qualified_csv_filename = qualify_company_events(\n",
    "        raw_company_events_filename, ai_company_qualifying_question\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"\\n\\n############\\nüßû‚Äç‚ôÇÔ∏è Extracting LinkedIn profiles from events...\\n############\")\n",
    "    linkedin_profiles_csv = extract_linkedin_profiles(\n",
    "        qualified_csv_filename, linkedin_titles\n",
    "    )\n",
    "    enriched_linkedin_profiles_csv = enrich_linkedin_profiles(linkedin_profiles_csv)\n",
    "\n",
    "    print(\n",
    "        \"\\n\\n############\\nü§ñ Performing GPT checks on LinkedIn profiles...\\n############\")\n",
    "    checked_linkedin_profiles_csv = perform_gpt_checks_on_profiles(\n",
    "        enriched_linkedin_profiles_csv, linkedin_titles\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n############\\nüóÑ Creating final filtered CSV...\\n############\")\n",
    "    final_csv_filename = create_final_filtered_csv(checked_linkedin_profiles_csv)\n",
    "\n",
    "    print(\"\\n\\n############\\nüí¨ Sending Slack notification...\\n############\")\n",
    "    send_slack_notification(\n",
    "        final_csv_filename, trigger_name, client_archetype_id, client_webhook_url\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n############\\nüì§ Generating Upload to Campaign CSV...\\n############\")\n",
    "    upload_to_campaign_csv = generate_upload_to_campaign_csv(final_csv_filename)\n",
    "    print(\n",
    "        \"Step 1: Upload CSV to the SellScale Campaign \"\n",
    "        + \"https://app.sellscale.com/contacts?campaign_id=\"\n",
    "        + str(client_archetype_id)\n",
    "    )\n",
    "    print(\n",
    "        \"Step 2: Go to Settings > Advanced > Custom Data Point and upload the CSV file there\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n############\\nUploading prospects to trigger run\\n############\")\n",
    "    upload_prospects_from_csv_to_campaign(\n",
    "        trigger_run_id, bearer_token, upload_to_campaign_csv\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n############\\n‚úÖ Trigger Run Done!\\n############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trigger: Recent Data Leak Companies - DevOps / Security Engineers\n",
      "{\n",
      "    \"ai_company_qualifying_question\": \"the company needs to be a tech company and not a non-profit or government organization\",\n",
      "    \"client_archetype_id\": 498,\n",
      "    \"client_webhook_url\": \"https://hooks.slack.com/services/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\",\n",
      "    \"linkedin_titles\": [\n",
      "        \"devops engineer\",\n",
      "        \"site reliability engineer\",\n",
      "        \"security engineer\"\n",
      "    ],\n",
      "    \"news_event_query\": \"data leak 'startup'\",\n",
      "    \"trigger_name\": \"Recent Data Leak Companies - DevOps / Security Engineers\"\n",
      "}\n",
      "\n",
      "\n",
      "############\n",
      "üì∞ Extracting company news events...\n",
      "############\n",
      "https://serpapi.com/search\n",
      "Data extraction completed. CSV file created: 20231124-160632-data leak 'startup'.csv\n",
      "Qualification completed. Updated CSV file created: 20231124-160632-data leak 'startup'-qualified.csv\n",
      "\n",
      "\n",
      "############\n",
      "üßû‚Äç‚ôÇÔ∏è Extracting LinkedIn profiles from events...\n",
      "############\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "LinkedIn profiles extraction completed. CSV file created: 20231124-160712-linkedin-profiles.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [04:32<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn profiles enrichment completed. Enriched CSV file created: 20231124-161145-enriched-linkedin-profiles.csv\n",
      "\n",
      "\n",
      "############\n",
      "ü§ñ Performing GPT checks on LinkedIn profiles...\n",
      "############\n",
      "Performing GPT checks on LinkedIn profiles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:44<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn profiles checking completed. Checked CSV file created: 20231124-161230-checked-linkedin-profiles.csv\n",
      "\n",
      "\n",
      "############\n",
      "üóÑ Creating final filtered CSV...\n",
      "############\n",
      "Final filtered CSV file created: 20231124-161230-final-filtered-profiles.csv\n",
      "\n",
      "\n",
      "############\n",
      "üí¨ Sending Slack notification...\n",
      "############\n",
      "Sending Slack message...\n",
      "Sent Slack message for event: Hackers are watching your startup. Not many are prepared for the attack\n",
      "Sent Slack message for event: Telehealth startup Cerebral had a HIPAA-violating data breach\n",
      "\n",
      "\n",
      "############\n",
      "üì§ Generating Upload to Campaign CSV...\n",
      "############\n",
      "Upload to Campaign CSV file created: 20231124-161231-upload-to-campaign.csv\n",
      "Step 1: Upload CSV to the SellScale Campaign https://app.sellscale.com/contacts?campaign_id=498\n",
      "Step 2: Go to Settings > Advanced > Custom Data Point and upload the CSV file there\n",
      "\n",
      "\n",
      "############\n",
      "Uploading prospects to trigger run\n",
      "############\n",
      "\n",
      "\n",
      "############\n",
      "‚úÖ Trigger Run Done!\n",
      "############\n"
     ]
    }
   ],
   "source": [
    "run_trigger(2, \"4kNfi28LT3buFoAmIb3xf9QtO9uYOCKm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
