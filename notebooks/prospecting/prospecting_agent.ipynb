{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI HELPER FUNCTIONS\n",
    "from ast import Dict, List\n",
    "from ctypes import Union\n",
    "import re\n",
    "from typing import Any, Dict, List, NewType, Optional, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from serpapi import GoogleSearch\n",
    "import openai\n",
    "\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "# map src\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.utils.slack import send_slack_message, URL_MAP\n",
    "\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "serp_api_key = os.environ.get(\"SERP_API_KEY\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_KEY\")\n",
    "\n",
    "env_path = Path(\"../..\") / \".envprod\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "ISCRAPER_API_KEY = os.environ.get(\"ISCRAPER_API_KEY\")\n",
    "PROFILE_DETAILS_URL = \"https://api.iscraper.io/v2/profile-details\"\n",
    "\n",
    "\n",
    "CURRENT_OPENAI_DAVINCI_MODEL = \"text-davinci-003\"\n",
    "CURRENT_OPENAI_CHAT_GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "CURRENT_OPENAI_LATEST_GPT_MODEL = \"gpt-4\"\n",
    "DEFAULT_SUFFIX = None\n",
    "DEFAULT_MAX_TOKENS = 16\n",
    "DEFAULT_TEMPERATURE = 1\n",
    "DEFAULT_TOP_P = 1\n",
    "DEFAULT_N = 1\n",
    "DEFAULT_FREQUENCY_PENALTY = 0\n",
    "DEFAULT_STOP = None\n",
    "\n",
    "NUM_GOOGLE_RESULTS_TO_SCRAPE = 10\n",
    "MAX_NUM_PROFILES_TO_PROCESS = 10\n",
    "\n",
    "\n",
    "def search_google_news(query, type: Optional[str] = None):\n",
    "    # https://support.google.com/websearch/answer/2466433?hl=en\n",
    "    params = {\n",
    "        \"api_key\": serp_api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"tbm\": type,\n",
    "        \"gl\": \"us\",  # US only\n",
    "        \"hl\": \"en\",\n",
    "        \"num\": NUM_GOOGLE_RESULTS_TO_SCRAPE,\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def wrapped_chat_gpt_completion(\n",
    "    messages: list,\n",
    "    history: Optional[list] = [],\n",
    "    max_tokens: Optional[int] = DEFAULT_MAX_TOKENS,\n",
    "    temperature: Optional[float] = DEFAULT_TEMPERATURE,\n",
    "    top_p: Optional[float] = DEFAULT_TOP_P,\n",
    "    n: Optional[int] = DEFAULT_N,\n",
    "    frequency_penalty: Optional[float] = DEFAULT_FREQUENCY_PENALTY,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a completion using the GPT-3.5-turbo model.\n",
    "\n",
    "    messages needs to be in the format:\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, how are you?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I am doing well, how about you?\"\n",
    "        }\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    if history:\n",
    "        messages = history + messages\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=CURRENT_OPENAI_LATEST_GPT_MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        n=n,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "    if response is None or response[\"choices\"] is None or len(response[\"choices\"]) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    choices = response[\"choices\"]\n",
    "    top_choice = choices[0]\n",
    "    preview = top_choice[\"message\"][\"content\"].strip()\n",
    "\n",
    "    messages = messages + [{\"role\": \"assistant\", \"content\": preview}]\n",
    "    return messages, preview\n",
    "\n",
    "\n",
    "def call_iscraper(linkedin_id: str):\n",
    "    payload = json.dumps(\n",
    "        {\n",
    "            \"profile_id\": linkedin_id,\n",
    "            \"profile_type\": \"personal\",\n",
    "            \"network_info\": True,\n",
    "        }\n",
    "    )\n",
    "    headers = {\"X-API-KEY\": ISCRAPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", PROFILE_DETAILS_URL, headers=headers, data=payload\n",
    "    )\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def research_corporate_profile_details(company_name: str):\n",
    "    payload = json.dumps(\n",
    "        {\n",
    "            \"profile_id\": company_name,\n",
    "            \"profile_type\": \"company\",\n",
    "            \"contact_info\": True,\n",
    "            \"recommendations\": True,\n",
    "            \"related_profiles\": True,\n",
    "            \"network_info\": True,\n",
    "        }\n",
    "    )\n",
    "    headers = {\"X-API-KEY\": ISCRAPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", PROFILE_DETAILS_URL, headers=headers, data=payload\n",
    "    )\n",
    "\n",
    "    return json.loads(response.text)\n",
    "\n",
    "\n",
    "def str_path_to_path_steps(path: str, *delimiters: str):\n",
    "    steps = re.split(\"|\".join(map(re.escape, delimiters)), path)\n",
    "    for step in steps:\n",
    "        if step.isdigit():\n",
    "            yield int(step)\n",
    "        else:\n",
    "            yield step\n",
    "\n",
    "\n",
    "def deep_get(obj: Union[List, Dict], path: str, default=None) -> Optional[Any]:\n",
    "    steps = str_path_to_path_steps(path, \".\")\n",
    "\n",
    "    for step in steps:\n",
    "        if not obj:\n",
    "            return default\n",
    "\n",
    "        if isinstance(obj, dict):\n",
    "            obj = obj.get(step, None)\n",
    "        elif isinstance(obj, list) and str(step).isnumeric():\n",
    "            obj = obj[int(step)]\n",
    "        else:\n",
    "            try:\n",
    "                obj = getattr(obj, step)\n",
    "            except:\n",
    "                return default\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################\n",
      "üí¨ Sending Slack notification...\n",
      "############################\n",
      "Sending Slack message...\n",
      "\n",
      "\n",
      "############################\n",
      "üì§ Generating Upload to Campaign CSV...\n",
      "############################\n",
      "Upload to Campaign CSV file created: 20231120-141948-upload-to-campaign.csv\n",
      "Step 1: Upload CSV to the SellScale Campaign https://app.sellscale.com/contacts?campaign_id=498\n",
      "Step 2: Go to Settings > Advanced > Custom Data Point and upload the CSV file there\n",
      "\n",
      "\n",
      "############################\n",
      "‚úÖ Trigger Run Done!\n",
      "############################\n"
     ]
    }
   ],
   "source": [
    "def extract_event_company_info(event_query: str):\n",
    "    # Fetch recent news articles related to the event\n",
    "    news_results = search_google_news(event_query, \"nws\")\n",
    "\n",
    "    # Prepare data for CSV\n",
    "    csv_data = []\n",
    "\n",
    "    for article in news_results.get(\"news_results\", []):\n",
    "        # Prepare the message for Chat GPT to extract the company name\n",
    "        chat_message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Which company does this news article pertain to? {article['title']} {article['snippet']}\\nOnly respond with the company name. If company not found, return 'none' all lowercase.\\nCompany name:\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Extract company name using Chat GPT\n",
    "        _, company_name = wrapped_chat_gpt_completion(chat_message)\n",
    "\n",
    "        # If company name is not found, skip the article\n",
    "        if \"none\" in company_name.lower():\n",
    "            continue\n",
    "\n",
    "        # Append the details to the CSV data list\n",
    "        csv_data.append(\n",
    "            {\n",
    "                \"img_url\": article.get(\"thumbnail\"),\n",
    "                \"title\": article.get(\"title\"),\n",
    "                \"snippet\": article.get(\"snippet\"),\n",
    "                \"link\": article.get(\"link\"),\n",
    "                \"date\": article.get(\"date\"),\n",
    "                \"company_name\": company_name,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Generate a timestamped CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # file name with timestamp, and the query\n",
    "    csv_filename = f\"{timestamp}-{event_query}.csv\"\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(\n",
    "            file,\n",
    "            fieldnames=[\"img_url\", \"title\", \"snippet\", \"link\", \"date\", \"company_name\"],\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_data)\n",
    "\n",
    "    print(f\"Data extraction completed. CSV file created: {csv_filename}\")\n",
    "\n",
    "    return csv_filename\n",
    "\n",
    "\n",
    "def qualify_company_events(csv_filename: str, ai_qualifying_question: str):\n",
    "    # Read the existing CSV file\n",
    "    with open(csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Process each row to add 'qualified' column\n",
    "    for row in data:\n",
    "        # Prepare the message for Chat GPT\n",
    "        chat_message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{ai_qualifying_question} Event: {row['title']}. Company: {row['company_name']}.\\nOnly respond with 'true' or 'false'.\\nResponse:\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Get qualification response using Chat GPT\n",
    "        _, qualification_response = wrapped_chat_gpt_completion(chat_message)\n",
    "\n",
    "        # Determine qualification (True/False)\n",
    "        qualified = \"true\" in qualification_response.lower()\n",
    "\n",
    "        # Add 'qualified' field to the row\n",
    "        row[\"qualified\"] = qualified\n",
    "\n",
    "    # Write the updated data to a new CSV file\n",
    "    new_csv_filename = csv_filename.replace(\".csv\", \"-qualified.csv\")\n",
    "    with open(new_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Qualification completed. Updated CSV file created: {new_csv_filename}\")\n",
    "\n",
    "    return new_csv_filename\n",
    "\n",
    "\n",
    "def extract_linkedin_profiles(qualified_csv_filename: str, titles: list):\n",
    "    # Read the qualified companies\n",
    "    with open(qualified_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        qualified_companies = [\n",
    "            row for row in reader if row[\"qualified\"].lower() == \"true\"\n",
    "        ]\n",
    "\n",
    "    # Prepare data for CSV\n",
    "    profiles_data = []\n",
    "\n",
    "    # Loop through each company and title\n",
    "    for company in qualified_companies:\n",
    "        for title in titles:\n",
    "            # Construct the Google search query\n",
    "            query = f'site:linkedin.com/in/ \"{company[\"company_name\"]}\" \"- {title}\"'\n",
    "\n",
    "            # Perform the Google search\n",
    "            search_results = search_google_news(\n",
    "                query\n",
    "            )  # Use your search_google_news function\n",
    "            organic_results = search_results.get(\"organic_results\", [])\n",
    "\n",
    "            # Process search results\n",
    "            for profile in organic_results:\n",
    "                # Extract relevant profile details\n",
    "                profile_data = {\n",
    "                    \"img_url\": company.get(\"img_url\"),  # Original data\n",
    "                    \"original_title\": company.get(\"title\"),  # Original data\n",
    "                    \"snippet\": company.get(\"snippet\"),  # Original data\n",
    "                    \"original_link\": company.get(\"link\"),  # Original data\n",
    "                    \"date\": company.get(\"date\"),  # Original data\n",
    "                    \"company_name\": company[\"company_name\"],  # Original data\n",
    "                    \"linkedin_title\": title,  # LinkedIn profile title\n",
    "                    \"profile_url\": profile.get(\"link\"),  # LinkedIn profile URL\n",
    "                }\n",
    "                profiles_data.append(profile_data)\n",
    "\n",
    "    # Generate a CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    csv_filename = f\"{timestamp}-linkedin-profiles.csv\"\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=profiles_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(profiles_data)\n",
    "\n",
    "    print(f\"LinkedIn profiles extraction completed. CSV file created: {csv_filename}\")\n",
    "\n",
    "    return csv_filename\n",
    "\n",
    "\n",
    "def enrich_linkedin_profiles(linkedin_csv_filename: str):\n",
    "    # Read the LinkedIn profiles CSV\n",
    "    with open(linkedin_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        linkedin_data = list(reader)\n",
    "\n",
    "    enriched_data = []\n",
    "\n",
    "    for row in tqdm(linkedin_data[0:MAX_NUM_PROFILES_TO_PROCESS]):\n",
    "        # Call the iScraper API\n",
    "        profile_url = row[\"profile_url\"]\n",
    "        profile_id = profile_url.split(\"/in/\")[1]\n",
    "        iscraper_response = call_iscraper(\n",
    "            profile_id\n",
    "        )  # Assuming this function is already defined\n",
    "        time.sleep(1)  # Wait for 1 second between API calls\n",
    "\n",
    "        # Extract required fields from the iScraper response\n",
    "        first_name = iscraper_response.get(\"first_name\", \"\")\n",
    "        last_name = iscraper_response.get(\"last_name\", \"\")\n",
    "        company = deep_get(\n",
    "            iscraper_response,\n",
    "            \"position_groups.0.profile_positions.0.company\",\n",
    "            default=\"\",\n",
    "        )\n",
    "        sub_title = iscraper_response.get(\"sub_title\", \"\")\n",
    "        summary = iscraper_response.get(\"summary\", \"\")\n",
    "        title = deep_get(\n",
    "            iscraper_response, \"position_groups.0.profile_positions.0.title\", default=\"\"\n",
    "        )\n",
    "        industry = iscraper_response.get(\"industry\", \"\")\n",
    "        profile_picture = iscraper_response.get(\"profile_picture\", \"\")\n",
    "        raw_json = json.dumps(iscraper_response)\n",
    "\n",
    "        # Prepare the enriched row\n",
    "        enriched_row = {\n",
    "            **row,\n",
    "            \"prospect_first_name\": first_name,\n",
    "            \"prospect_last_name\": last_name,\n",
    "            \"prospect_company\": company,\n",
    "            \"prospect_sub_title\": sub_title,\n",
    "            \"prospect_summary\": summary,\n",
    "            \"prospect_title\": title,\n",
    "            \"prospect_industry\": industry,\n",
    "            \"profile_picture\": profile_picture,\n",
    "            \"raw_iscraper_json\": raw_json,\n",
    "        }\n",
    "\n",
    "        enriched_data.append(enriched_row)\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    enriched_csv_filename = f\"{timestamp}-enriched-linkedin-profiles.csv\"\n",
    "\n",
    "    # Write the data to the new CSV file\n",
    "    with open(enriched_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=enriched_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(enriched_data)\n",
    "\n",
    "    print(\n",
    "        f\"LinkedIn profiles enrichment completed. Enriched CSV file created: {enriched_csv_filename}\"\n",
    "    )\n",
    "\n",
    "    return enriched_csv_filename\n",
    "\n",
    "\n",
    "def perform_gpt_checks_on_profiles(enriched_csv_filename: str, roles: list):\n",
    "    # Read the enriched LinkedIn profiles CSV\n",
    "    with open(enriched_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        enriched_data = list(reader)\n",
    "\n",
    "    checked_data = []\n",
    "\n",
    "    print(\"Performing GPT checks on LinkedIn profiles...\")\n",
    "\n",
    "    for row in tqdm(enriched_data):\n",
    "        # Check if the company matches (case-insensitive)\n",
    "        correct_company = (\n",
    "            row[\"company_name\"].lower() == row[\"prospect_company\"].lower()\n",
    "            or row[\"company_name\"].lower() in row[\"prospect_company\"].lower()\n",
    "            or row[\"prospect_company\"].lower() in row[\"company_name\"].lower()\n",
    "        )\n",
    "        row[\"correct_company\"] = correct_company\n",
    "\n",
    "        # Prepare the message for Chat GPT to check the role\n",
    "        chat_message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Does the role '{row['prospect_title']}' match any of these roles: {roles}?\\nOnly respond with 'true' or 'false'.\\nResponse:\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Get the role match response using Chat GPT\n",
    "        _, role_match_response = wrapped_chat_gpt_completion(chat_message)\n",
    "\n",
    "        # Determine if the role matches (True/False)\n",
    "        correct_role = \"true\" in role_match_response.lower()\n",
    "        row[\"correct_role\"] = correct_role\n",
    "\n",
    "        checked_data.append(row)\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    checked_csv_filename = f\"{timestamp}-checked-linkedin-profiles.csv\"\n",
    "\n",
    "    # Write the data to the new CSV file\n",
    "    with open(checked_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=checked_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(checked_data)\n",
    "\n",
    "    print(\n",
    "        f\"LinkedIn profiles checking completed. Checked CSV file created: {checked_csv_filename}\"\n",
    "    )\n",
    "\n",
    "    return checked_csv_filename\n",
    "\n",
    "\n",
    "def create_final_filtered_csv(checked_csv_filename: str):\n",
    "    # Read the checked LinkedIn profiles CSV\n",
    "    with open(checked_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        checked_data = list(reader)\n",
    "\n",
    "    # Filter data where both correct_role and correct_company are True\n",
    "    filtered_data = [\n",
    "        row\n",
    "        for row in checked_data\n",
    "        if row[\"correct_role\"].lower() == \"true\"\n",
    "        and row[\"correct_company\"].lower() == \"true\"\n",
    "    ]\n",
    "\n",
    "    # remove duplicates on linkedin url\n",
    "    filtered_data = [dict(t) for t in {tuple(d.items()) for d in filtered_data}]\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    final_csv_filename = f\"{timestamp}-final-filtered-profiles.csv\"\n",
    "\n",
    "    # Write the filtered data to the new CSV file\n",
    "    with open(final_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=filtered_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(filtered_data)\n",
    "\n",
    "    print(f\"Final filtered CSV file created: {final_csv_filename}\")\n",
    "\n",
    "    return final_csv_filename\n",
    "\n",
    "\n",
    "def send_slack_notification(\n",
    "    final_csv_filename: str, trigger_name: str, client_archetype_id: int\n",
    "):\n",
    "    # Read data from the final CSV\n",
    "    with open(final_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Prepare sample events for the Slack message\n",
    "    sample_events = []\n",
    "    for row in data:\n",
    "        event = {\n",
    "            \"title\": row[\"original_title\"],\n",
    "            \"company\": row[\"company_name\"],\n",
    "            'url': row['original_link'],\n",
    "            \"prospects\": [\n",
    "                {\n",
    "                    \"name\": f\"{row['prospect_first_name']} {row['prospect_last_name']}\",\n",
    "                    \"linkedin_url\": row[\"profile_url\"],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        # if original title already exists, append to prospects\n",
    "        for sample_event in sample_events:\n",
    "            if sample_event[\"title\"] == event[\"title\"]:\n",
    "                sample_event[\"prospects\"].append(event[\"prospects\"][0])\n",
    "                break\n",
    "        else:\n",
    "            sample_events.append(event)\n",
    "\n",
    "    # Send Slack message\n",
    "    print(\"Sending Slack message...\")\n",
    "    result = send_slack_message(\n",
    "        message=\"hello\",\n",
    "        webhook_urls=[URL_MAP[\"eng-sandbox\"]],\n",
    "        blocks=[\n",
    "            {\n",
    "                \"type\": \"header\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"plain_text\",\n",
    "                    \"text\": \"Trigger ‚ö°Ô∏è: \" + trigger_name,\n",
    "                    \"emoji\": True,\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"section\",\n",
    "                \"text\": {\n",
    "                    \"type\": \"mrkdwn\",\n",
    "                    \"text\": f\"Identified {len(data)} prospects at {len(set(row['company_name'] for row in data))} companies with recent news\",\n",
    "                },\n",
    "            },\n",
    "            {\"type\": \"divider\"},\n",
    "            *[\n",
    "                {\n",
    "                    \"type\": \"section\",\n",
    "                    \"text\": {\n",
    "                        \"type\": \"mrkdwn\",\n",
    "                        \"text\": f\"- *{event['company'].replace('.', ' ')}: _<{event['url']}|'{event['title'][0:50] + ('...' if len(event['title']) > 50 else '')}'>_*:\\n\\t\\t- {len(event['prospects'])} prospects like <{event['prospects'][0]['linkedin_url']}|{event['prospects'][0]['name']}> were identified\",\n",
    "                    },\n",
    "                }\n",
    "                for event in sample_events\n",
    "            ],\n",
    "            # button to go to campaign\n",
    "            {\n",
    "                \"type\": \"actions\",\n",
    "                \"elements\": [\n",
    "                    {\n",
    "                        \"type\": \"button\",\n",
    "                        \"text\": {\"type\": \"plain_text\", \"text\": \"View Prospects\"},\n",
    "                        \"url\": f\"https://app.sellscale.com/contacts?campaign_id={client_archetype_id}\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_upload_to_campaign_csv(final_csv_filename: str):\n",
    "    # Read data from the final CSV\n",
    "    with open(final_csv_filename, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Prepare data for the upload_to_campaign CSV\n",
    "    campaign_data = []\n",
    "    for row in data:\n",
    "        campaign_row = {\n",
    "            \"first_name\": row[\"prospect_first_name\"],\n",
    "            \"last_name\": row[\"prospect_last_name\"],\n",
    "            \"linkedin_url\": row[\"profile_url\"],\n",
    "            \"company\": row[\"company_name\"],\n",
    "            \"custom_data\": f\"{row['company_name']} was recently in the news titled '{row['original_title']}'\",\n",
    "        }\n",
    "        campaign_data.append(campaign_row)\n",
    "\n",
    "    # Generate a new CSV file name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    campaign_csv_filename = f\"{timestamp}-upload-to-campaign.csv\"\n",
    "\n",
    "    # Write the campaign data to the new CSV file\n",
    "    with open(campaign_csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=campaign_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(campaign_data)\n",
    "\n",
    "    print(f\"Upload to Campaign CSV file created: {campaign_csv_filename}\")\n",
    "\n",
    "    return campaign_csv_filename\n",
    "\n",
    "\n",
    "# INPUTS\n",
    "trigger_name = \"Recent Data Leak Companies - DevOps / Security Engineers\"\n",
    "news_event_query = \"data leak\"\n",
    "ai_company_qualifying_question = \"the company needs to be a tech company and not a non-profit or government organization\"\n",
    "linkedin_titles = [\"devops engineer\", \"site reliability engineer\", \"security engineer\"]\n",
    "client_archetype_id = 498\n",
    "\n",
    "# OUTPUTS\n",
    "# Example usage\n",
    "print(\n",
    "    \"\\n\\n############################\\nüì∞ Extracting company news events...\\n############################\")\n",
    "raw_company_events_filename = extract_event_company_info(news_event_query)\n",
    "qualified_csv_filename = qualify_company_events(\n",
    "    raw_company_events_filename, ai_company_qualifying_question\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\\n############################\\nüßû‚Äç‚ôÇÔ∏è Extracting LinkedIn profiles from events...\\n############################\")\n",
    "linkedin_profiles_csv = extract_linkedin_profiles(\n",
    "    qualified_csv_filename, linkedin_titles\n",
    ")\n",
    "enriched_linkedin_profiles_csv = enrich_linkedin_profiles(linkedin_profiles_csv)\n",
    "\n",
    "print(\n",
    "    \"\\n\\n############################\\nü§ñ Performing GPT checks on LinkedIn profiles...\\n############################\")\n",
    "checked_linkedin_profiles_csv = perform_gpt_checks_on_profiles(\n",
    "    enriched_linkedin_profiles_csv, linkedin_titles\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\\n############################\\nüóÑ Creating final filtered CSV...\\n############################\")\n",
    "final_csv_filename = create_final_filtered_csv(checked_linkedin_profiles_csv)\n",
    "\n",
    "print(\n",
    "    \"\\n\\n############################\\nüí¨ Sending Slack notification...\\n############################\")\n",
    "send_slack_notification(final_csv_filename, trigger_name, client_archetype_id)\n",
    "\n",
    "print(\"\\n\\n############################\\nüì§ Generating Upload to Campaign CSV...\\n############################\")\n",
    "upload_to_campaign_csv = generate_upload_to_campaign_csv(final_csv_filename)\n",
    "print(\"Step 1: Upload CSV to the SellScale Campaign \" + \"https://app.sellscale.com/contacts?campaign_id=\" + str(client_archetype_id))\n",
    "print(\"Step 2: Go to Settings > Advanced > Custom Data Point and upload the CSV file there\")\n",
    "\n",
    "print(\n",
    "    \"\\n\\n############################\\n‚úÖ Trigger Run Done!\\n############################\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
